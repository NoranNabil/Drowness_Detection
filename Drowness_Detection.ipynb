{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fj9YcAnsT4B_"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6pCmkJrUC9g"
      },
      "source": [
        "## Helper Functions\n",
        "Below are a few helper function to make converting between different image data types and formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "09b_0FAnUa9y"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQyo0k2kOvyY"
      },
      "source": [
        "## Haar Cascade Classifier\n",
        "For this tutorial we will run a simple object detection algorithm called Haar Cascade on our images and video fetched from our webcam. OpenCV has a pre-trained Haar Cascade face detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ukPempXNOw76"
      },
      "outputs": [],
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_xcqQZKYzAj"
      },
      "source": [
        "## Webcam Videos\n",
        "Running code on webcam video is a little more complex than images. We need to start a video stream using our webcam as input. Then we run each frame through our progam (Drowness detection).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ghUlAJzKSjFT"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rI46WpcvVrF5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TrFP11FvWHoA"
      },
      "outputs": [],
      "source": [
        "SIZE=224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "W3a91F11WLrs"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "VGG16_model=load_model('/content/drive/MyDrive/ai-iot-project/ai/weights/Vgg16_cropped_final.h5') ## Replace with the path of the weights\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKg_bomEII7K",
        "outputId": "17f71486-8086-41a6-a960-477c93360c38"
      },
      "outputs": [],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "nZ7ddA3HIJwB"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from mtcnn import MTCNN\n",
        "\n",
        "def crop_face_and_return(image):\n",
        "    cropped_face = None\n",
        "    detector = MTCNN()\n",
        "    faces = detector.detect_faces(image)\n",
        "    if faces:\n",
        "\n",
        "      x, y, width, height = faces[0]['box']\n",
        "\n",
        "      cropped_face = image[y:y+height, x:x+width]\n",
        "\n",
        "    return cropped_face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nkSnkbkk4cC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "filenames=['/content/image_1','/content/image_2','/content/image_3','/content/image_4','/content/image_5']\n",
        "predictions=[]\n",
        "i=0\n",
        "while True and i<5:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    cv2.imwrite(filenames[i]+'.jpg', img)\n",
        "\n",
        "\n",
        "    image = cv2.imread(filenames[i]+'.jpg')\n",
        "    cropped_face = crop_face_and_return(image)\n",
        "    if cropped_face is not None:\n",
        "    # Convert the NumPy array 'cropped_face' into a PIL Image\n",
        "        pil_image = Image.fromarray(cropped_face,'RGB')\n",
        "\n",
        "        pil_image = pil_image.resize((SIZE, SIZE))\n",
        "\n",
        "        cropped_face = np.array(pil_image)\n",
        "\n",
        "        image=tf.reshape(cropped_face , (1, SIZE,SIZE, 3))\n",
        "\n",
        "        predictions.append(VGG16_model.predict(image))\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    i+=1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "peYF1wxuss-4"
      },
      "outputs": [],
      "source": [
        "predictions=[np.argmax(pr) for pr in predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeDGQatysxKv",
        "outputId": "8f779787-fb05-4163-a2e4-78d0f91d909d"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8-PAAfyaG7s"
      },
      "source": [
        "### **VIEDO TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXmHCxOAfiLW",
        "outputId": "05bbf183-cc87-403a-a6fa-64268cfcd80f"
      },
      "outputs": [],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eqRjb5ViflMN"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from mtcnn import MTCNN\n",
        "\n",
        "def crop_face_and_return(image):\n",
        "    cropped_face = None\n",
        "    detector = MTCNN()\n",
        "    faces = detector.detect_faces(image)\n",
        "    if faces:\n",
        "\n",
        "      x, y, width, height = faces[0]['box']\n",
        "\n",
        "      cropped_face = image[y:y+height, x:x+width]\n",
        "\n",
        "    return cropped_face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDqjsDTe8yHT",
        "outputId": "fd2a66cb-b5cc-4a4f-b283-2f32f7d966f0"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpT0t0AG8_fm",
        "outputId": "ed8e3195-30d6-4d75-b0ca-b7cdab4ab697"
      },
      "outputs": [],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgdUvpg99EyW"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "import pandas as pd\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/nikospetrellis/nitymed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nLjyEcqUCwR6"
      },
      "outputs": [],
      "source": [
        "SIZE=224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "ehKChYadC0bH"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "VGG16_model=load_model('/content/drive/MyDrive/ai-iot-project/ai/weights/Vgg16_cropped_final.h5') ## Replace with the path of the weights\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "eVUPBhjrrRna"
      },
      "outputs": [],
      "source": [
        "import matplotlib.image as mpimg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlsaU8U08eZ8"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Open the video file\n",
        "\n",
        "video_path = '/content/nitymed/DSM_Dataset-HDTV720/Yawning/Male/HDTV720/P1043124_720.mp4'  # Replace with the path to your video file\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Set the frame rate and interval\n",
        "frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
        "interval = int(frame_rate * 3)  # Capture frame every 3 seconds\n",
        "\n",
        "# Initialize variables\n",
        "frame_count = 0\n",
        "image_count = 1\n",
        "\n",
        "predictions=[]\n",
        "\n",
        "\n",
        "while (True) and (image_count<6)  :\n",
        "    # Read the next frame\n",
        "    ret, frame = video.read()\n",
        "\n",
        "    # Break the loop if no more frames are available\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    # Save image every 2 seconds\n",
        "    if frame_count % interval == 0:\n",
        "        # Save the image\n",
        "        image_path = f'/content/frame_{image_count}.jpg'\n",
        "        cv2.imwrite(image_path, frame)\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        cropped_face = crop_face_and_return(image)\n",
        "        if cropped_face is not None:\n",
        "\n",
        "          pil_image = Image.fromarray(cropped_face)\n",
        "\n",
        "          pil_image = pil_image.resize((SIZE, SIZE))\n",
        "\n",
        "\n",
        "          image=tf.reshape(pil_image, (1, SIZE,SIZE, 3))\n",
        "          predictions.append(VGG16_model.predict(image))\n",
        "\n",
        "\n",
        "        image_count += 1\n",
        "# Release the video\n",
        "video.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "UQum78sCQ2BU"
      },
      "outputs": [],
      "source": [
        "predictions =[np.argmax(pr) for pr in  predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XkgiUsSRObS"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfanOOP8amsT"
      },
      "source": [
        "## **Send the prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dapq3D2syqmC",
        "outputId": "9c4b6481-3e6e-42c3-f324-d0ae476f00e3"
      },
      "outputs": [],
      "source": [
        "out=\"Safe.\"\n",
        "for i in range (0,len(predictions)-1):\n",
        "    if  (predictions[i]==1 and predictions[i+1] ==1) or ((predictions[i]==1 and predictions[i+1] ==2)or(predictions[i]==2 and predictions[i+1] ==1)):\n",
        "      out=\"\".join(\"DROWSY DRIVER, Wake up!!\")\n",
        "      break\n",
        "    elif (predictions[i]==2 and predictions[i+1] ==2):\n",
        "      out=\"\".join(\"You are going to sleep, drink coffee please!!\")\n",
        "      break\n",
        "    else :\n",
        "      continue\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tahjH0guaikb",
        "outputId": "cc90e3f6-f448-4a84-fc58-961ff82d26ec"
      },
      "outputs": [],
      "source": [
        "!pip install paho-mqtt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgTQPBWEatqO",
        "outputId": "00a45de4-bc77-4a4b-d1ce-b9ecfa8891f7"
      },
      "outputs": [],
      "source": [
        "import paho.mqtt.client as mqtt #import the client1\n",
        "broker_address=\"mqtt-dashboard.com\" ### set your broker address here\n",
        "#broker_address=\"iot.eclipse.org\" #use external broker\n",
        "client = mqtt.Client(\"P1\") #create new instance\n",
        "client.connect(broker_address) #connect to broker\n",
        "client.publish(\"NTI/final\",out)#publish ### set your topic name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2KEH3bch83x",
        "outputId": "b035443f-4a55-453c-9c40-2f4aaaef9817"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
